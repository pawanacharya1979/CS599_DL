{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOadshdBpfY+RrS8QL6/9zC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawanacharya1979/CS599_DL/blob/update_branch/CS_599_Lab3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CS599: Foundations of Deep Learning ‚Äì Assignment #00011\n",
        "**Topic:** Batch, Weight, and Layer Normalization using TensorFlow 2\n",
        "\n",
        "What are we going to cover:\n",
        "- Implementations of Batch Normalization, Weight Normalization, and Layer Normalization.\n",
        "- Integration of these normalization methods into a simple CNN.\n",
        "- Training using `tf.GradientTape` and comparison with built-in TensorFlow normalization functions.\n"
      ],
      "metadata": {
        "id": "WovSi2I_atJz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization**"
      ],
      "metadata": {
        "id": "djjfwsxYgKTb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization (BN) is designed to stabilize and accelerate the training of neural networks by normalizing the activations across the mini-batch. The main idea is to transform the inputs to each layer such that they have zero mean and unit variance, which reduces issues related to internal covariate shift (i.e., the distribution change of layer inputs during training)."
      ],
      "metadata": {
        "id": "CD3K6QlugOz7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process**:\n",
        "\n",
        "For each feature in the neural network, BN computes the mean and variance over the current mini-batch. Then, it normalizes the feature by subtracting the mean and dividing by the standard deviation. However, simply forcing all outputs to have zero mean and unit variance might limit the network‚Äôs expressive ability. To solve this, BN introduces two learnable parameters:\n",
        "\n",
        "**Scaling factor** (ùõæ):\n",
        "\n",
        "Controls the magnitude of the normalized activations.\n",
        "\n",
        "**Shifting factor** (ùõΩ):\n",
        "\n",
        "Allows an offset so that the output can recenter the activations as necessary.\n",
        "\n",
        "This means that although the network starts with standardized activations, the layer can adapt to recover any useful distribution by learning\n",
        "ùõæ and ùõΩ."
      ],
      "metadata": {
        "id": "G6wq2zSCgQK8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose we have a mini-batch { ùë• 1 , ùë• 2 , ‚Ä¶ , ùë• ùëÅ } {x 1 ‚Äã ,x 2 ‚Äã ,‚Ä¶,x N ‚Äã } for a given input feature:"
      ],
      "metadata": {
        "id": "7jQ9vPWRg-o0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mini-batch Mean:**\n",
        "\n",
        "$$\n",
        "\\mu_{MB} = \\frac{1}{N}\\sum_{i=1}^{N} x_i\n",
        "$$\n",
        "\n",
        "Here, \\(\\mu_{MB}\\) is the average of the feature values in the mini-batch.\n",
        "\n",
        "**Mini-batch Variance:**\n",
        "\n",
        "$$\n",
        "\\sigma_{MB}^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\left( x_i - \\mu_{MB} \\right)^2\n",
        "$$\n",
        "\n",
        "The variance measures the spread of the feature values around the mean.\n",
        "\n",
        "**Normalization:**\n",
        "\n",
        "$$\n",
        "\\hat{x}_i = \\frac{x_i - \\mu_{MB}}{\\sqrt{\\sigma_{MB}^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "The small constant \\(\\epsilon\\) is added for numerical stability, ensuring that you do not divide by zero.\n",
        "\n",
        "**Scale and Shift:**\n",
        "\n",
        "$$\n",
        "z_i = \\gamma \\hat{x}_i + \\beta\n",
        "$$\n",
        "\n",
        "Here, \\(\\gamma\\) and \\(\\beta\\) are learnable parameters, so the network can adjust the normalized output to any optimal distribution.\n",
        "\n"
      ],
      "metadata": {
        "id": "zjRSVIkDhX-B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_mean = tf.reduce_mean(x, axis=axes, keepdims=True)\n",
        "\n",
        "batch_variance = tf.reduce_mean(tf.square(x - batch_mean), axis=axes, keepdims=True)\n",
        "\n",
        "x_norm = (x - batch_mean) / tf.sqrt(batch_variance + epsilon)\n",
        "\n",
        "return gamma * x_norm + beta\n"
      ],
      "metadata": {
        "id": "mCX3gM5XiCwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How does BN work with ReLU?**\n",
        "\n",
        "In practice, Batch Normalization is typically applied before the activation function (e.g., ReLU). This means that while BN normalizes the input to have a mean of 0 and a unit variance, the ReLU activation (which zeroes out negative values) is then applied to these normalized values. The learnable parameters\n",
        "ùõæ and ùõΩ help adjust the normalized output so that even after ReLU‚Äôs non-linearity, the network retains the capacity to learn optimal representations."
      ],
      "metadata": {
        "id": "qZv72Gabi7Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Increased Weight Magnitudes**:\n",
        "\n",
        "One might think that simply increasing weight magnitude improves convergence. However, this can lead to instability during training. BN counters this by normalizing the activations regardless of the weight magnitude. Then, the learnable ùõæ (which scales the normalized activations) effectively allows the model to ‚Äúrecover‚Äù the needed weight magnitude if that is optimal for the task. Similarly, ùõΩ provides a trainable offset. Thus, while BN keeps the activations in a ‚Äúnice‚Äù range, it doesn‚Äôt restrict the network‚Äôs ability to learn a suitable transformation."
      ],
      "metadata": {
        "id": "gTVpUDITjG4n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weight Normalization**"
      ],
      "metadata": {
        "id": "5v2fbDofiJ5C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Normalization is a technique used to reparameterize the weights of a neural network layer. Instead of directly learning a weight\n",
        "ùë§, we decompose it into two separate components:\n",
        "\n",
        "*   ùë£: A vector that defines the direction of the weight.\n",
        "*   ùëî: A scalar that controls the magnitude (scale) of the weight.\n",
        "\n"
      ],
      "metadata": {
        "id": "7eI3G_rKkZ4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In standard neural network layers, the weight\n",
        "ùë§ carries both its scale and direction. By decoupling these into\n",
        "ùë£ and ùëî, the optimization process can update the direction and magnitude independently. This separation often results in:\n",
        "\n",
        "**Faster convergence**:\n",
        "\n",
        "Because the optimizer can adjust the scale without affecting the weight‚Äôs direction.\n",
        "\n",
        "**Greater stability**:\n",
        "\n",
        "Since fluctuations in magnitude and direction don‚Äôt interfere with each other.\n",
        "\n"
      ],
      "metadata": {
        "id": "S4sz7zd2krFP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Typical Post-activation**\n",
        "\n",
        "In a standard neural network layer, the output is computed as:\n",
        "\n",
        "$$\n",
        "Y = \\phi(W \\cdot x + b)\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\(Y\\) is the output after applying an activation function \\(\\phi\\) (e.g., ReLU),\n",
        "- \\(W\\) is the weight matrix,\n",
        "- \\(x\\) is the input, and\n",
        "- \\(b\\) is the bias vector.\n",
        "\n",
        "**Weight Normalization**\n",
        "\n",
        "Weight Normalization reparameterizes each weight vector \\(w\\) (a row or column of \\(W\\)) as follows:\n",
        "\n",
        "$$\n",
        "w = \\frac{g}{\\|v\\|} \\, v\n",
        "$$\n",
        "\n",
        "where:\n",
        "- \\(v\\) is a learnable vector representing the *direction* of the weight,\n",
        "- \\(g\\) is a learnable scalar representing the desired norm of the weight, and\n",
        "- \\(\\|v\\|\\) is the Euclidean (L2) norm of \\(v\\).\n",
        "\n",
        "The Euclidean norm is computed by:\n",
        "\n",
        "$$\n",
        "\\|v\\| = \\sqrt{\\sum_{i=1}^{k} v_i^2}\n",
        "$$\n",
        "\n",
        "if \\(v\\) is \\(k\\)-dimensional.\n"
      ],
      "metadata": {
        "id": "ofoKfmYTlHQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reparameterization:**\n",
        "\n",
        "Instead of optimizing \\(w\\) directly, we define \\(w\\) as:\n",
        "\n",
        "$$\n",
        "w = \\frac{g}{\\|v\\|} \\, v\n",
        "$$\n",
        "\n",
        "This means the network now has two sets of parameters:\n",
        "- \\(v\\): the raw weight direction\n",
        "- \\(g\\): the scaling factor\n"
      ],
      "metadata": {
        "id": "NmpE3r2vlu0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimization:**\n",
        "\n",
        "During training, gradient-based optimizers (such as SGD or Adam) update\n",
        "ùë£ and ùëî independently.\n",
        "\n",
        "The reparameterization ensures that even if the original weight\n",
        "ùë§ might change in scale, its direction is controlled by ùë£ and can be maintained, while ùëî sets the overall magnitude."
      ],
      "metadata": {
        "id": "65Emujbal7c5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Impact:**\n",
        "\n",
        "**Decoupled Updates**: The learning process can adjust ùëî to find an appropriate scale without affecting the gradient flow related to the direction ùë£.\n",
        "\n",
        "**Stable Convergence**: With independent control, the optimizer is less likely to encounter issues related to too-large or too-small weight magnitudes."
      ],
      "metadata": {
        "id": "9GoVfrOZmcnf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits:**\n",
        "\n",
        "*   Independent control over the weight's scale and direction.\n",
        "*   Improved gradient flow and potentially accelerated training.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ga_OxaqgnJlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization**"
      ],
      "metadata": {
        "id": "wDvNrFbznunj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Normalization (LN) is a normalization technique that standardizes the inputs across the features for each individual training example. Unlike Batch Normalization (BN), which normalizes each feature over a mini-batch, LN computes the mean and variance over the features of each sample separately."
      ],
      "metadata": {
        "id": "R-bjNOsLnxp9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Independence from Batch Size:**\n",
        "\n",
        "Since LN does not depend on the statistics of the mini-batch, it is especially useful when dealing with small batches or models like recurrent neural networks (RNNs) where the notion of a ‚Äúbatch‚Äù may not apply across time steps.\n",
        "\n",
        "**Stability Across Samples:**\n",
        "\n",
        "LN reduces internal covariate shift at the level of each sample, which can lead to more stable training when the distribution of features varies significantly among samples."
      ],
      "metadata": {
        "id": "zRQlJvScn0_M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Normalization follows a similar idea to Batch Normalization, but instead of computing statistics across a mini-batch, it computes them for every single sample across its feature dimensions."
      ],
      "metadata": {
        "id": "Qk7DFMlan9FP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1: Compute the Mean for Each Sample**\n",
        "\n",
        "For each sample \\(i\\) (where the sample contains features \\(x_{ij}\\) for \\(j = 1, \\dots, H\\)), the mean is computed as:\n",
        "\n",
        "$$\n",
        "\\mu_i = \\frac{1}{H}\\sum_{j=1}^{H} x_{ij}\n",
        "$$\n",
        "\n",
        "**Step 2: Compute the Variance for Each Sample**\n",
        "\n",
        "The variance for each sample \\(i\\) is computed as:\n",
        "\n",
        "$$\n",
        "\\sigma_i^2 = \\frac{1}{H}\\sum_{j=1}^{H} \\left(x_{ij} - \\mu_i\\right)^2\n",
        "$$\n",
        "\n",
        "**Step 3: Normalize the Features**\n",
        "\n",
        "Normalize each feature \\(x_{ij}\\) in the sample using:\n",
        "\n",
        "$$\n",
        "\\hat{x}_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}\n",
        "$$\n",
        "\n",
        "where \\(\\epsilon\\) is a small constant for numerical stability.\n",
        "\n",
        "**Step 4: Scale and Shift the Normalized Data**\n",
        "\n",
        "Finally, apply learnable parameters \\(\\gamma\\) (scale) and \\(\\beta\\) (shift) for each feature:\n",
        "\n",
        "$$\n",
        "z_{ij} = \\gamma\\, \\hat{x}_{ij} + \\beta\n",
        "$$\n"
      ],
      "metadata": {
        "id": "BOWK2KlJoO2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison with Batch Normalization:**"
      ],
      "metadata": {
        "id": "mAlPYHndoXAs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Batch Normalization:**\n",
        "\n",
        "**Mean Computation:**\n",
        "\n",
        "Across the mini-batch for each feature, the mean is computed as:\n",
        "\n",
        "$$\n",
        "\\mu_j = \\frac{1}{N}\\sum_{i=1}^{N} x_{ij}\n",
        "$$\n",
        "\n",
        "**Variance Computation:**\n",
        "\n",
        "Also computed over the mini-batch for each feature:\n",
        "\n",
        "$$\n",
        "\\sigma_j^2 = \\frac{1}{N}\\sum_{i=1}^{N} \\left( x_{ij} - \\mu_j \\right)^2\n",
        "$$\n"
      ],
      "metadata": {
        "id": "IELokvj3olIL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer Normalization:**\n",
        "\n",
        "**Mean Computation:** For each sample across its features:\n",
        "\n",
        "$$\n",
        "\\mu_i = \\frac{1}{H}\\sum_{j=1}^{H} x_{ij}\n",
        "$$\n",
        "\n",
        "**Variance Computation:** Also computed for each sample:\n",
        "\n",
        "$$\n",
        "\\sigma_i^2 = \\frac{1}{H}\\sum_{j=1}^{H} \\left(x_{ij} - \\mu_i\\right)^2\n",
        "$$\n"
      ],
      "metadata": {
        "id": "ssrtkloKo0vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The key difference is the dimension over which the statistics are computed: BN normalizes each feature over the batch (and possibly spatial dimensions in a CNN), whereas LN normalizes across the features for each individual sample."
      ],
      "metadata": {
        "id": "G-vAqjdUo6_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Benefits:**\n",
        "\n",
        "Layer Normalization is particularly beneficial when batch sizes are small or in recurrent architectures where per-sample normalization can improve training stability."
      ],
      "metadata": {
        "id": "-arKKmP3pF7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preparation"
      ],
      "metadata": {
        "id": "uYLJbTW2csZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Verify TensorFlow version\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "# Load Fashion MNIST and prepare data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "x_train = np.expand_dims(x_train.astype(np.float32) / 255.0, -1)\n",
        "x_test = np.expand_dims(x_test.astype(np.float32) / 255.0, -1)\n",
        "print(\"Train shape:\", x_train.shape, \"Test shape:\", x_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KBPZV5eEcouS",
        "outputId": "f25f5c46-0625-4169-e259-124edb6ae54c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.18.0\n",
            "Train shape: (60000, 28, 28, 1) Test shape: (10000, 28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization Functions"
      ],
      "metadata": {
        "id": "7mX8JvCUdV2t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch Normalization\n",
        "def batch_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    axes = list(range(len(x.shape) - 1))\n",
        "    batch_mean = tf.reduce_mean(x, axis=axes, keepdims=True)\n",
        "    batch_variance = tf.reduce_mean(tf.square(x - batch_mean), axis=axes, keepdims=True)\n",
        "    x_norm = (x - batch_mean) / tf.sqrt(batch_variance + epsilon)\n",
        "    return gamma * x_norm + beta\n",
        "\n",
        "# Weight Normalization\n",
        "def weight_norm(v, g, axis=None, epsilon=1e-5):\n",
        "    v_norm = tf.sqrt(tf.reduce_sum(tf.square(v), axis=axis, keepdims=True) + epsilon)\n",
        "    return (g / v_norm) * v\n",
        "\n",
        "# Layer Normalization\n",
        "def layer_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
        "    x_norm = (x - mean) / tf.sqrt(variance + epsilon)\n",
        "    return gamma * x_norm + beta\n"
      ],
      "metadata": {
        "id": "Vf4kEK7hdWvw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN with Normalization Options"
      ],
      "metadata": {
        "id": "iTu_MPYEdk7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=10, norm_type='batch'):\n",
        "        super(CNN, self).__init__()\n",
        "        self.norm_type = norm_type\n",
        "        self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', use_bias=True)\n",
        "        if self.norm_type == 'batch':\n",
        "            self.gamma_bn = tf.Variable(tf.ones([1, 1, 1, 32]), trainable=True)\n",
        "            self.beta_bn = tf.Variable(tf.zeros([1, 1, 1, 32]), trainable=True)\n",
        "        elif self.norm_type == 'layer':\n",
        "            self.gamma_ln = tf.Variable(tf.ones([32]), trainable=True)\n",
        "            self.beta_ln = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.dense = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv1(x)\n",
        "        if self.norm_type == 'batch':\n",
        "            x = batch_norm(x, self.gamma_bn, self.beta_bn)\n",
        "        elif self.norm_type == 'layer':\n",
        "            shape = tf.shape(x)\n",
        "            x_reshaped = tf.reshape(x, [-1, x.shape[-1]])\n",
        "            x = tf.reshape(layer_norm(x_reshaped, self.gamma_ln, self.beta_ln), shape)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.flatten(x)\n",
        "        return self.dense(x)\n",
        "\n",
        "# Create model instances\n",
        "model_bn = CNN(norm_type='batch')\n",
        "model_ln = CNN(norm_type='layer')\n",
        "model_no_norm = CNN(norm_type='none')\n"
      ],
      "metadata": {
        "id": "f3eyDrJ4dn0Z"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Setup and Step Function"
      ],
      "metadata": {
        "id": "78Vahjpqd2ZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions = model(images, training=True)\n",
        "        loss = loss_object(labels, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "3erWPUamd4a1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the Model (Example with Batch Normalization)"
      ],
      "metadata": {
        "id": "lo39rpH-d-n0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model_bn(images, training=False)\n",
        "    test_accuracy.update_state(labels, predictions)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy.result().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDDnAZNbeCnh",
        "outputId": "82b478bd-c6ba-4316-ae28-ed77b7cd7d88"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate on Test Data"
      ],
      "metadata": {
        "id": "pqqSmaRMech2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "for images, labels in test_dataset:\n",
        "    predictions = model_bn(images, training=False)\n",
        "    test_accuracy.update_state(labels, predictions)\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy.result().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iu8Ki7SedZQ",
        "outputId": "ad428b83-2093-456e-a9d8-030643c29ba8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.0891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare BatchNorm with Built-In BatchNormalization"
      ],
      "metadata": {
        "id": "yT9gsAubekBe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_input = tf.random.normal([32, 28, 28, 32])\n",
        "custom_output = batch_norm(sample_input, tf.ones([1,1,1,32]), tf.zeros([1,1,1,32]))\n",
        "bn_layer = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-5)\n",
        "tf_output = bn_layer(sample_input, training=True)\n",
        "difference = tf.reduce_mean(tf.abs(custom_output - tf_output))\n",
        "print(\"Mean absolute difference:\", difference.numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miWWUo8AenKG",
        "outputId": "36828b0f-314a-4261-8d18-a186063285f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean absolute difference: 4.420937e-08\n"
          ]
        }
      ]
    }
  ]
}