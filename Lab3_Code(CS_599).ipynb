{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0SgY0X3EAKwZDAtMDImq6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pawanacharya1979/CS599_DL/blob/main/Lab3_Code(CS_599).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtOq8yN1prfx",
        "outputId": "bda30a02-aabf-4e7e-b68f-16d76967ec5b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 (Layer Norm): Loss = 0.4115\n",
            "Epoch 2 (Layer Norm): Loss = 0.3032\n",
            "Epoch 3 (Layer Norm): Loss = 0.2819\n",
            "Epoch 4 (Layer Norm): Loss = 0.2648\n",
            "Epoch 5 (Layer Norm): Loss = 0.2533\n",
            "Test Accuracy (Layer Norm): 89.21%\n",
            "Mean absolute difference between custom BN and TF BN: 0.000000\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 1. Custom Normalization Functions\n",
        "# -----------------------------------------------------------\n",
        "def custom_batch_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Batch Normalization.\n",
        "    Computes the mean and variance over the mini-batch (and spatial dimensions for CNNs)\n",
        "    then normalizes and rescales the input.\n",
        "\n",
        "    x: Input tensor.\n",
        "    gamma: Learnable scale parameter.\n",
        "    beta: Learnable shift parameter.\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    axes = list(range(len(x.shape) - 1))  # Normalize over batch, height, and width dimensions\n",
        "    batch_mean = tf.reduce_mean(x, axis=axes, keepdims=True)\n",
        "    batch_variance = tf.reduce_mean(tf.square(x - batch_mean), axis=axes, keepdims=True)\n",
        "    x_norm = (x - batch_mean) / tf.sqrt(batch_variance + epsilon)\n",
        "    return gamma * x_norm + beta\n",
        "\n",
        "def custom_weight_norm(v, g, axis=None, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Weight Normalization.\n",
        "    Reparameterizes a weight vector v using a scalar g such that:\n",
        "         w = (g / ||v||) * v,\n",
        "    where ||v|| is the Euclidean norm of v.\n",
        "\n",
        "    v: Weight vector (or tensor).\n",
        "    g: Learnable scalar (or tensor, broadcastable to v) controlling the magnitude.\n",
        "    axis: The axis (or axes) along which to compute the norm.\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    v_norm = tf.sqrt(tf.reduce_sum(tf.square(v), axis=axis, keepdims=True) + epsilon)\n",
        "    return (g / v_norm) * v\n",
        "\n",
        "def custom_layer_norm(x, gamma, beta, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Custom Layer Normalization.\n",
        "    Normalizes the input tensor x across the feature dimension for each sample.\n",
        "\n",
        "    x: Input tensor with shape [..., features].\n",
        "    gamma: Learnable scale parameter (broadcastable to x).\n",
        "    beta: Learnable shift parameter (broadcastable to x).\n",
        "    epsilon: Small constant for numerical stability.\n",
        "    \"\"\"\n",
        "    mean = tf.reduce_mean(x, axis=-1, keepdims=True)\n",
        "    variance = tf.reduce_mean(tf.square(x - mean), axis=-1, keepdims=True)\n",
        "    x_norm = (x - mean) / tf.sqrt(variance + epsilon)\n",
        "    return gamma * x_norm + beta\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 2. CNN Model with Normalization Options\n",
        "# -----------------------------------------------------------\n",
        "class CustomCNN(tf.keras.Model):\n",
        "    def __init__(self, num_classes=10, norm_type='none'):\n",
        "        \"\"\"\n",
        "        A CNN model that applies the specified normalization in the forward pass.\n",
        "\n",
        "        norm_type: 'batch', 'layer', 'weight', or 'none'\n",
        "        \"\"\"\n",
        "        super(CustomCNN, self).__init__()\n",
        "        self.norm_type = norm_type\n",
        "\n",
        "        # For standard convolution (if not using weight normalization)\n",
        "        if self.norm_type != 'weight':\n",
        "            self.conv1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding='same', use_bias=True)\n",
        "        else:\n",
        "            # For weight normalization, initialize kernel components separately.\n",
        "            self.kernel_shape = (3, 3, 1, 32)\n",
        "            self.v = tf.Variable(tf.random.normal(self.kernel_shape, stddev=0.1), trainable=True)\n",
        "            self.g = tf.Variable(tf.ones((1,)), trainable=True)\n",
        "            self.conv1_bias = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "\n",
        "        # For Batch Normalization: create learnable gamma and beta parameters.\n",
        "        if self.norm_type == 'batch':\n",
        "            self.gamma_bn = tf.Variable(tf.ones([1, 1, 1, 32]), trainable=True)\n",
        "            self.beta_bn  = tf.Variable(tf.zeros([1, 1, 1, 32]), trainable=True)\n",
        "        # For Layer Normalization: create gamma and beta per channel.\n",
        "        elif self.norm_type == 'layer':\n",
        "            self.gamma_ln = tf.Variable(tf.ones([32]), trainable=True)\n",
        "            self.beta_ln  = tf.Variable(tf.zeros([32]), trainable=True)\n",
        "\n",
        "        self.pool1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
        "        self.flatten = tf.keras.layers.Flatten()\n",
        "        self.fc = tf.keras.layers.Dense(num_classes)\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        # Convolution operation.\n",
        "        if self.norm_type == 'weight':\n",
        "            kernel = custom_weight_norm(self.v, self.g, axis=[0, 1, 2])\n",
        "            conv = tf.nn.conv2d(x, kernel, strides=1, padding='SAME') + self.conv1_bias\n",
        "        else:\n",
        "            conv = self.conv1(x)\n",
        "\n",
        "        # Apply normalization based on the chosen norm_type.\n",
        "        if self.norm_type == 'batch':\n",
        "            conv = custom_batch_norm(conv, self.gamma_bn, self.beta_bn)\n",
        "        elif self.norm_type == 'layer':\n",
        "            # Reshape to merge spatial dimensions for layer normalization.\n",
        "            shape = tf.shape(conv)\n",
        "            conv_reshaped = tf.reshape(conv, [shape[0], -1, conv.shape[-1]])\n",
        "            conv_norm = custom_layer_norm(conv_reshaped, self.gamma_ln, self.beta_ln)\n",
        "            conv = tf.reshape(conv_norm, shape)\n",
        "        # If norm_type is 'none', nothing extra is done.\n",
        "\n",
        "        # Activation and pooling.\n",
        "        x_act = tf.nn.relu(conv)\n",
        "        x_pool = self.pool1(x_act)\n",
        "        x_flat = self.flatten(x_pool)\n",
        "        logits = self.fc(x_flat)\n",
        "        return logits\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 3. Training Setup: Loss, Optimizer, and Training Step\n",
        "# -----------------------------------------------------------\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, images, labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(images, training=True)\n",
        "        loss = loss_object(labels, logits)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "# -----------------------------------------------------------\n",
        "# 4. Main Function: Data Preparation, Training, Evaluation, and Comparison\n",
        "# -----------------------------------------------------------\n",
        "def main():\n",
        "    # Data Preparation: Load Fashion MNIST dataset.\n",
        "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "    x_train = np.expand_dims(x_train.astype(np.float32) / 255.0, -1)\n",
        "    x_test = np.expand_dims(x_test.astype(np.float32) / 255.0, -1)\n",
        "\n",
        "    batch_size = 64\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(batch_size)\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Instantiate a model.\n",
        "    # Change norm_type to one of 'batch', 'layer', 'weight', or 'none' as desired.\n",
        "    model = CustomCNN(norm_type='layer')\n",
        "\n",
        "    # Build the model by calling it on a dummy input.\n",
        "    dummy_input = tf.random.normal([1, 28, 28, 1])\n",
        "    _ = model(dummy_input, training=True)\n",
        "\n",
        "    # Training Loop.\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        for images, labels in train_dataset:\n",
        "            loss = train_step(model, images, labels)\n",
        "            total_loss += loss\n",
        "            steps += 1\n",
        "        print(f\"Epoch {epoch+1} (Layer Norm): Loss = {total_loss/steps:.4f}\")\n",
        "\n",
        "    # Evaluation on the Test Data.\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(batch_size)\n",
        "    test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "    for images, labels in test_dataset:\n",
        "        logits = model(images, training=False)\n",
        "        test_accuracy.update_state(labels, logits)\n",
        "    print(f\"Test Accuracy (Layer Norm): {test_accuracy.result().numpy()*100:.2f}%\")\n",
        "\n",
        "    # -----------------------------------------------------------\n",
        "    # 5. Comparison with TensorFlow Built-In Normalization Functions\n",
        "    # -----------------------------------------------------------\n",
        "    sample_input = tf.random.normal([32, 28, 28, 32])\n",
        "    custom_bn_output = custom_batch_norm(sample_input,\n",
        "                                         gamma=tf.ones([1,1,1,32]),\n",
        "                                         beta=tf.zeros([1,1,1,32]))\n",
        "    bn_layer = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-5)\n",
        "    tf_bn_output = bn_layer(sample_input, training=True)\n",
        "    difference = tf.reduce_mean(tf.abs(custom_bn_output - tf_bn_output))\n",
        "    print(f\"Mean absolute difference between custom BN and TF BN: {difference.numpy():.6f}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    }
  ]
}